import os
import pandas as pd
import time
import logging
import random
from requests.exceptions import RequestException, ProxyError, ConnectionError, Timeout, HTTPError, SSLError
from Crawl99designEntry import download_images

# è®¾ç½®æ—¥å¿—è®°å½•
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler('contest_crawler.log'),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger(__name__)

def download_contest_with_retry(contest_url, contest_output_dir, csv_filename, contest_id, contest_name, max_retries=20, base_delay=3):
    """
    å¸¦é‡è¯•æœºåˆ¶çš„æ¯”èµ›ä¸‹è½½å‡½æ•°ï¼Œä¸“é—¨å¤„ç†ç½‘ç»œè¿æ¥é—®é¢˜
    
    Args:
        contest_url: æ¯”èµ›URL
        contest_output_dir: è¾“å‡ºç›®å½•
        csv_filename: CSVæ–‡ä»¶å
        contest_id: æ¯”èµ›ID
        contest_name: æ¯”èµ›åç§°
        max_retries: æœ€å¤§é‡è¯•æ¬¡æ•°ï¼Œé»˜è®¤20æ¬¡
        base_delay: åŸºç¡€å»¶è¿Ÿæ—¶é—´ï¼ˆç§’ï¼‰ï¼Œé»˜è®¤3ç§’
    
    Returns:
        tuple: (success: bool, error_message: str)
    """
    
    # å®šä¹‰éœ€è¦é‡è¯•çš„å¼‚å¸¸ç±»å‹
    retryable_exceptions = (
        ProxyError, 
        ConnectionError, 
        Timeout, 
        HTTPError,
        SSLError,
        RequestException
    )
    
    last_error = None
    
    for attempt in range(max_retries + 1):
        try:
            logger.info(f"å°è¯•ç¬¬ {attempt + 1}/{max_retries + 1} æ¬¡ä¸‹è½½æ¯”èµ›: {contest_name} (ID: {contest_id})")
            
            # åœ¨é‡è¯•å‰æ·»åŠ éšæœºå»¶è¿Ÿï¼Œé¿å…è¯·æ±‚è¿‡äºå¯†é›†
            if attempt > 0:
                # æŒ‡æ•°é€€é¿ + éšæœºæŠ–åŠ¨
                base_wait = min(base_delay * (2 ** (attempt - 1)), 300)  # æœ€å¤§å»¶è¿Ÿ5åˆ†é’Ÿ
                jitter = random.uniform(0.5, 1.5)  # æ·»åŠ éšæœºæŠ–åŠ¨
                wait_time = base_wait * jitter
                
                logger.info(f"ç­‰å¾… {wait_time:.1f} ç§’åé‡è¯•...")
                time.sleep(wait_time)
            
            # è°ƒç”¨ä¸‹è½½å‡½æ•°
            download_images(contest_url, contest_output_dir, csv_filename, nonactive=True)
            
            # æ£€æŸ¥æ˜¯å¦æˆåŠŸç”ŸæˆCSVæ–‡ä»¶
            contest_csv_path = os.path.join(contest_output_dir, f"Submission_Contestant_{csv_filename}.csv")
            
            if os.path.exists(contest_csv_path):
                # æ£€æŸ¥CSVæ–‡ä»¶æ˜¯å¦æœ‰æœ‰æ•ˆå†…å®¹
                try:
                    df = pd.read_csv(contest_csv_path)
                    if len(df) > 0:
                        # è¿›ä¸€æ­¥æ£€æŸ¥æ•°æ®è´¨é‡
                        valid_entries = df[df['DesignID'].notna() & (df['DesignID'] != 'N/A')]
                        if len(valid_entries) > 0:
                            logger.info(f" ä¸‹è½½æˆåŠŸ: {contest_name} (ID: {contest_id}), è·å¾— {len(df)} æ¡è®°å½• ({len(valid_entries)} æœ‰æ•ˆ)")
                            return True, None
                        else:
                            logger.warning(f"CSVæ–‡ä»¶æ— æœ‰æ•ˆæ•°æ®: {contest_name} (ID: {contest_id})")
                    else:
                        logger.warning(f"CSVæ–‡ä»¶ä¸ºç©º: {contest_name} (ID: {contest_id})")
                        
                except Exception as csv_error:
                    logger.warning(f"CSVæ–‡ä»¶è¯»å–å¤±è´¥: {contest_name} (ID: {contest_id}), é”™è¯¯: {csv_error}")
                
                # å¦‚æœCSVå­˜åœ¨ä½†æ— æ•ˆï¼Œåˆ é™¤å®ƒä»¥ä¾¿é‡è¯•
                if attempt < max_retries:
                    try:
                        os.remove(contest_csv_path)
                        logger.info(f"åˆ é™¤æ— æ•ˆCSVæ–‡ä»¶ï¼Œå‡†å¤‡é‡è¯•")
                    except:
                        pass
            else:
                logger.warning(f"æœªç”ŸæˆCSVæ–‡ä»¶: {contest_name} (ID: {contest_id})")
            
            # å¦‚æœåˆ°è¿™é‡Œè¯´æ˜æœ¬æ¬¡å°è¯•å¤±è´¥ï¼Œè®°å½•é”™è¯¯ä¿¡æ¯
            last_error = "ä¸‹è½½æœªå®Œæˆæˆ–æ•°æ®æ— æ•ˆ"
            
        except retryable_exceptions as e:
            last_error = str(e)
            error_type = type(e).__name__
            
            logger.warning(f" {error_type} (ç¬¬{attempt + 1}æ¬¡å°è¯•): {contest_name} (ID: {contest_id})")
            logger.warning(f"é”™è¯¯è¯¦æƒ…: {last_error}")
            
            # å¯¹äºç‰¹å®šçš„é”™è¯¯ç±»å‹ï¼Œç»™å‡ºæ›´è¯¦ç»†çš„æ—¥å¿—
            if isinstance(e, ProxyError):
                logger.warning("ä»£ç†è¿æ¥é”™è¯¯ - å¯èƒ½æ˜¯ç½‘ç»œä¸ç¨³å®šæˆ–ä»£ç†æœåŠ¡å™¨é—®é¢˜")
            elif isinstance(e, ConnectionError):
                logger.warning("è¿æ¥é”™è¯¯ - å¯èƒ½æ˜¯ç½‘ç»œä¸­æ–­æˆ–æœåŠ¡å™¨æ— å“åº”")
            elif isinstance(e, Timeout):
                logger.warning("è¶…æ—¶é”™è¯¯ - ç½‘ç»œå»¶è¿Ÿè¿‡é«˜")
            elif isinstance(e, SSLError):
                logger.warning("SSLé”™è¯¯ - å¯èƒ½æ˜¯è¯ä¹¦é—®é¢˜")
            
            # ç‰¹æ®Šå¤„ç†ï¼šå¦‚æœæ˜¯è¿æ¥é‡ç½®é”™è¯¯ï¼Œå¢åŠ é¢å¤–å»¶è¿Ÿ
            if "Connection reset by peer" in str(e) or "ConnectionResetError" in str(e):
                if attempt < max_retries:
                    extra_delay = random.uniform(5, 10)
                    logger.info(f"æ£€æµ‹åˆ°è¿æ¥é‡ç½®ï¼Œé¢å¤–ç­‰å¾… {extra_delay:.1f} ç§’...")
                    time.sleep(extra_delay)
            
        except Exception as e:
            # å¯¹äºä¸å¯é‡è¯•çš„å¼‚å¸¸ï¼Œç›´æ¥è¿”å›å¤±è´¥
            last_error = str(e)
            error_type = type(e).__name__
            logger.error(f" ä¸å¯é‡è¯•çš„{error_type}é”™è¯¯: {contest_name} (ID: {contest_id})")
            logger.error(f"é”™è¯¯è¯¦æƒ…: {last_error}")
            return False, f"ä¸å¯é‡è¯•é”™è¯¯ ({error_type}): {last_error}"
    
    # å¦‚æœæ‰€æœ‰é‡è¯•éƒ½å¤±è´¥äº†
    logger.error(f" é‡è¯•æ¬¡æ•°å·²è€—å°½: {contest_name} (ID: {contest_id})")
    logger.error(f"æœ€åçš„é”™è¯¯: {last_error}")
    return False, f"é‡è¯•æ¬¡æ•°å·²è€—å°½ï¼Œæœ€åé”™è¯¯: {last_error}"

def check_contest_completion(contest_output_dir, csv_filename):
    """
    æ£€æŸ¥æ¯”èµ›æ˜¯å¦å·²ç»å®Œå…¨ä¸‹è½½å®Œæˆ
    
    Args:
        contest_output_dir: æ¯”èµ›è¾“å‡ºç›®å½•
        csv_filename: CSVæ–‡ä»¶å
    
    Returns:
        tuple: (is_complete: bool, entry_count: int, valid_count: int)
    """
    contest_csv_path = os.path.join(contest_output_dir, f"Submission_Contestant_{csv_filename}.csv")
    
    if not os.path.exists(contest_csv_path):
        return False, 0, 0
    
    try:
        df = pd.read_csv(contest_csv_path)
        entry_count = len(df)
        
        if entry_count > 0:
            # æ£€æŸ¥å…³é”®åˆ—æ˜¯å¦å­˜åœ¨
            required_columns = ['DesignID', 'Entry', 'UserID']
            if all(col in df.columns for col in required_columns):
                # æ£€æŸ¥æœ‰æ•ˆæ¡ç›®æ•°é‡
                valid_entries = df[
                    df['DesignID'].notna() & 
                    (df['DesignID'] != 'N/A') & 
                    (df['DesignID'] != '')
                ]
                valid_count = len(valid_entries)
                
                if valid_count > 0:
                    return True, entry_count, valid_count
        
        return False, entry_count, 0
        
    except Exception as e:
        logger.error(f"æ£€æŸ¥CSVæ–‡ä»¶æ—¶å‡ºé”™: {contest_csv_path}, é”™è¯¯: {e}")
        return False, 0, 0

def save_progress(progress_file, contest_id):
    """ä¿å­˜è¿›åº¦åˆ°æ–‡ä»¶"""
    try:
        with open(progress_file, 'a', encoding='utf-8') as f:
            f.write(f"{contest_id}\n")
    except Exception as e:
        logger.error(f"ä¿å­˜è¿›åº¦å¤±è´¥: {e}")

def load_progress(progress_file):
    """åŠ è½½å·²å®Œæˆçš„æ¯”èµ›IDåˆ—è¡¨"""
    completed = set()
    if os.path.exists(progress_file):
        try:
            with open(progress_file, 'r', encoding='utf-8') as f:
                completed = set(line.strip() for line in f if line.strip())
        except Exception as e:
            logger.error(f"åŠ è½½è¿›åº¦æ–‡ä»¶å¤±è´¥: {e}")
    return completed

def main():
    # Set base path
    output_dir_list = "/Users/samxie/Research/CrowdDeleRej/Data/ContestList/"  # Path to save contest list CSV
    output_dir_Image = "/Users/samxie/Research/CrowdDeleRej/Data/ImageList/0828/"
    
    # Ensure output directories exist
    os.makedirs(output_dir_Image, exist_ok=True)  # Create directory to save images
    
    # Define path for the aggregated CSV file
    all_contests_csv = os.path.join(output_dir_Image, "ContestEntry_20250828.csv")  # Path for aggregated CSV file
    
    # å¤±è´¥è®°å½•æ–‡ä»¶
    failed_contests_csv = os.path.join(output_dir_Image, "Failed_contests_entry.csv")
    
    # è¿›åº¦è®°å½•æ–‡ä»¶
    progress_file = os.path.join(output_dir_Image, "crawl_progress.txt")
    
    logger.info(" å¼€å§‹çˆ¬è™«ä»»åŠ¡...")
    
    # Step 2: Load the contest data CSV file that was scraped
    contests_df_path = os.path.join(output_dir_list, "Contest_URL_All_0828.csv")  # Path to scraped results CSV
    contests_df = pd.read_csv(contests_df_path)
    contests_df = contests_df.astype(str)  # Convert all columns to string type
    
    total_contests = len(contests_df)
    logger.info(f" æ€»å…±éœ€è¦å¤„ç† {total_contests} ä¸ªæ¯”èµ›")
    
    # If the aggregated CSV file already exists, load existing data; otherwise, create an empty DataFrame
    if os.path.exists(all_contests_csv):
        all_contests_df = pd.read_csv(all_contests_csv)
        logger.info(f" åŠ è½½äº†å·²å­˜åœ¨çš„èšåˆæ–‡ä»¶ï¼ŒåŒ…å« {len(all_contests_df)} æ¡è®°å½•")
    else:
        all_contests_df = pd.DataFrame()
        logger.info(" åˆ›å»ºæ–°çš„èšåˆDataFrame")
    
    # åŠ è½½å¤±è´¥è®°å½•
    failed_contests = []
    if os.path.exists(failed_contests_csv):
        failed_df = pd.read_csv(failed_contests_csv)
        logger.info(f"  åŠ è½½äº† {len(failed_df)} æ¡å¤±è´¥è®°å½•")
    else:
        failed_df = pd.DataFrame(columns=['ContestID', 'ContestName', 'ContestURL', 'FailureReason', 'AttemptTime'])
    
    # åŠ è½½è¿›åº¦è®°å½•
    completed_contests = load_progress(progress_file)
    logger.info(f" å·²å®Œæˆ {len(completed_contests)} ä¸ªæ¯”èµ›")
    
    # ç»Ÿè®¡ä¿¡æ¯
    successful_downloads = 0
    failed_downloads = 0
    skipped_downloads = 0
    
    # Step 4: Iterate through each contest and download images
    for index, row in contests_df.iterrows():
        contest_url = row['ContestURL']
        contest_id = row['ContestID']
        contest_name = row['ContestName']
        
        logger.info(f" å¤„ç†è¿›åº¦: {index + 1}/{total_contests} - {contest_name} (ID: {contest_id})")
        
        # æ£€æŸ¥æ˜¯å¦å·²ç»å®Œæˆ
        if contest_id in completed_contests:
            logger.info(f"â­  æ¯”èµ›å·²å®Œæˆï¼Œè·³è¿‡: {contest_name} (ID: {contest_id})")
            skipped_downloads += 1
            continue
        
        # Create a specific directory for each contest to save images
        contest_output_dir = os.path.join(output_dir_Image, contest_id)  # Path to save each contest's images
        os.makedirs(contest_output_dir, exist_ok=True)
        
        # æ£€æŸ¥æ˜¯å¦å·²ç»æœ‰å®Œæ•´çš„æ•°æ®
        csv_filename = f"{contest_id}_contest"  # Filename to save contest images CSV
        is_complete, entry_count, valid_count = check_contest_completion(contest_output_dir, csv_filename)
        
        if is_complete:
            logger.info(f" æ•°æ®å·²å®Œæ•´ï¼Œç›´æ¥åˆå¹¶: {contest_name} (ID: {contest_id}) - {entry_count} æ¡è®°å½• ({valid_count} æœ‰æ•ˆ)")
            
            # ç›´æ¥åŠ è½½å¹¶åˆå¹¶æ•°æ®
            contest_csv_path = os.path.join(contest_output_dir, f"Submission_Contestant_{csv_filename}.csv")
            contest_df = pd.read_csv(contest_csv_path)
            all_contests_df = pd.concat([all_contests_df, contest_df], ignore_index=True)
            
            # ä¿å­˜è¿›åº¦
            save_progress(progress_file, contest_id)
            successful_downloads += 1
            
            # å®šæœŸä¿å­˜èšåˆæ•°æ®
            if successful_downloads % 5 == 0:
                all_contests_df.to_csv(all_contests_csv, index=False)
                logger.info(f" å·²ä¿å­˜ä¸­é—´è¿›åº¦ï¼Œç´¯è®¡æˆåŠŸ {successful_downloads} ä¸ªæ¯”èµ›")
            
            continue
        
        # Step 5: Download images for the contest with retry mechanism
        logger.info(f"ğŸ”„ å¼€å§‹ä¸‹è½½æ¯”èµ›: {contest_name} (ID: {contest_id})...")
        
        download_success, error_message = download_contest_with_retry(
            contest_url, 
            contest_output_dir, 
            csv_filename,
            contest_id,
            contest_name,
            max_retries=20,  # æœ€å¤šé‡è¯•20æ¬¡
            base_delay=2     # åŸºç¡€å»¶è¿Ÿ3ç§’
        )
        
        if download_success:
            successful_downloads += 1
            logger.info(f" æ¯”èµ›ä¸‹è½½æˆåŠŸ: {contest_name} (ID: {contest_id})")
            
            # Step 6: Load the image data CSV for the current contest and merge it into the aggregated DataFrame
            contest_csv_path = os.path.join(contest_output_dir, f"Submission_Contestant_{csv_filename}.csv")
            
            if os.path.exists(contest_csv_path):
                try:
                    contest_df = pd.read_csv(contest_csv_path)
                    all_contests_df = pd.concat([all_contests_df, contest_df], ignore_index=True)
                    
                    # ä¿å­˜è¿›åº¦
                    save_progress(progress_file, contest_id)
                    
                    logger.info(f" å·²åˆå¹¶æ•°æ®: {len(contest_df)} æ¡è®°å½•")
                    
                    # å®šæœŸä¿å­˜èšåˆæ•°æ®ï¼Œé˜²æ­¢æ•°æ®ä¸¢å¤±
                    if successful_downloads % 5 == 0:
                        all_contests_df.to_csv(all_contests_csv, index=False)
                        logger.info(f" å·²ä¿å­˜ä¸­é—´è¿›åº¦ï¼Œç´¯è®¡æˆåŠŸ {successful_downloads} ä¸ªæ¯”èµ›")
                        
                except Exception as e:
                    logger.error(f"åˆå¹¶æ•°æ®æ—¶å‡ºé”™: {contest_name} (ID: {contest_id}), é”™è¯¯: {e}")
            
        else:
            failed_downloads += 1
            current_time = pd.Timestamp.now().strftime("%Y-%m-%d %H:%M:%S")
            
            # è®°å½•å¤±è´¥çš„æ¯”èµ›
            failed_contest_info = {
                'ContestID': contest_id,
                'ContestName': contest_name,
                'ContestURL': contest_url,
                'FailureReason': error_message,
                'AttemptTime': current_time
            }
            failed_contests.append(failed_contest_info)
            logger.error(f" è®°å½•å¤±è´¥æ¯”èµ›: {contest_name} (ID: {contest_id}) - {error_message}")
        
        # æ¯å¤„ç†10ä¸ªæ¯”èµ›ä¿å­˜ä¸€æ¬¡å¤±è´¥è®°å½•
        if len(failed_contests) >= 10:
            try:
                failed_df_temp = pd.DataFrame(failed_contests)
                failed_df = pd.concat([failed_df, failed_df_temp], ignore_index=True)
                failed_df.to_csv(failed_contests_csv, index=False)
                failed_contests = []  # æ¸…ç©ºä¸´æ—¶åˆ—è¡¨
                logger.info(f" å·²ä¿å­˜å¤±è´¥è®°å½•ï¼Œå½“å‰æ€»å¤±è´¥æ•°é‡: {failed_downloads}")
            except Exception as e:
                logger.error(f"ä¿å­˜å¤±è´¥è®°å½•æ—¶å‡ºé”™: {e}")
        
        # æ·»åŠ è¯·æ±‚é—´éš”ï¼Œé¿å…è¿‡äºé¢‘ç¹çš„è¯·æ±‚
        time.sleep(random.uniform(1, 3))
    
    # ä¿å­˜æœ€ç»ˆçš„å¤±è´¥è®°å½•
    if failed_contests:
        try:
            failed_df_temp = pd.DataFrame(failed_contests)
            failed_df = pd.concat([failed_df, failed_df_temp], ignore_index=True)
            failed_df.to_csv(failed_contests_csv, index=False)
            logger.info(f" ä¿å­˜äº†æœ€ç»ˆçš„å¤±è´¥è®°å½•")
        except Exception as e:
            logger.error(f"ä¿å­˜æœ€ç»ˆå¤±è´¥è®°å½•æ—¶å‡ºé”™: {e}")
    
    # Step 7: Final save of the aggregated data and remove duplicates
    if not all_contests_df.empty:
        try:
            logger.info(" æ­£åœ¨å¤„ç†æœ€ç»ˆæ•°æ®...")
            
            # Remove duplicates based on DesignID
            original_count = len(all_contests_df)
            all_contests_df.drop_duplicates(subset='DesignID', keep='first', inplace=True)
            deduplicated_count = len(all_contests_df)
            
            if original_count > deduplicated_count:
                logger.info(f"ğŸ§¹ å»é‡å®Œæˆ: ç§»é™¤äº† {original_count - deduplicated_count} æ¡é‡å¤è®°å½•")
            
            # Sort by ContestID (descending) and Entry (descending within each ContestID)
            all_contests_df.sort_values(by=['ContestID', 'Entry'], ascending=[False, False], inplace=True)
            
            # æœ€ç»ˆä¿å­˜
            all_contests_df.to_csv(all_contests_csv, index=False)
            logger.info(f" èšåˆCSVæ–‡ä»¶å·²ä¿å­˜: {all_contests_csv}")
            
        except Exception as e:
            logger.error(f"å¤„ç†æœ€ç»ˆæ•°æ®æ—¶å‡ºé”™: {e}")
    
    # è¾“å‡ºæœ€ç»ˆç»Ÿè®¡ä¿¡æ¯
    logger.info("="*60)
    logger.info(" çˆ¬å–ä»»åŠ¡å®Œæˆç»Ÿè®¡:")
    logger.info(f" æ€»æ¯”èµ›æ•°é‡: {total_contests}")
    logger.info(f" æˆåŠŸä¸‹è½½: {successful_downloads}")
    logger.info(f" å¤±è´¥ä¸‹è½½: {failed_downloads}")
    logger.info(f"â­  è·³è¿‡ä¸‹è½½: {skipped_downloads}")
    
    if total_contests > 0:
        success_rate = successful_downloads / total_contests * 100
        logger.info(f"ğŸ¯ æˆåŠŸç‡: {success_rate:.2f}%")
    
    if failed_downloads > 0:
        logger.info(f"ğŸ“‹ å¤±è´¥è®°å½•å·²ä¿å­˜åˆ°: {failed_contests_csv}")
    
    logger.info("="*60)
    print(f"Aggregated CSV file saved as {all_contests_csv}")

if __name__ == "__main__":
    try:
        main()
    except KeyboardInterrupt:
        logger.info(" ç”¨æˆ·ä¸­æ–­äº†ç¨‹åº")
    except Exception as e:
        logger.error(f" ç¨‹åºå‘ç”Ÿæœªé¢„æœŸçš„é”™è¯¯: {e}")
        import traceback
        logger.error(traceback.format_exc())